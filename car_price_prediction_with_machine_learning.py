# -*- coding: utf-8 -*-
"""Car Price Prediction with Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sDX6LrSM_X-iaf0RpnluMCQVIY-IPtzz
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# --- 1. Load the Dataset ---
print("--- 1. Loading the Dataset ---")
df = pd.read_csv('car data.csv')
print("First 5 rows of the dataset:")
print(df.head())
print("\nDataset Info:")
print(df.info())
print("-" * 40 + "\n")

# --- 2. Data Preprocessing and Feature Engineering ---
print("--- 2. Data Preprocessing and Feature Engineering ---")

# Check for missing values
print("Missing values before preprocessing:")
print(df.isnull().sum())

# Calculate the current year (assuming the current year is 2025 as per previous turns)
current_year = 2025

# Create a new feature 'Car_Age'
df['Car_Age'] = current_year - df['Year']

# Drop the original 'Year' column and 'Car_Name' column as they are not needed for the model
# 'Car_Name' is too specific and generally not useful for a generalized price prediction model
df = df.drop(['Year', 'Car_Name'], axis=1)

print("\nDataFrame after adding 'Car_Age' and dropping 'Year' and 'Car_Name':")
print(df.head())

# Get unique values for categorical features to understand their categories
print("\nUnique values for 'Fuel_Type':", df['Fuel_Type'].unique())
print("Unique values for 'Selling_type':", df['Selling_type'].unique())
print("Unique values for 'Transmission':", df['Transmission'].unique())

# Apply one-hot encoding to categorical features
# drop_first=True helps to avoid multicollinearity
df = pd.get_dummies(df, columns=['Fuel_Type', 'Selling_type', 'Transmission'], drop_first=True)

print("\nDataFrame after one-hot encoding:")
print(df.head())
print("-" * 40 + "\n")

# --- 3. Exploratory Data Analysis (Optional but Recommended) ---
# This part is not strictly required by the prompt, but it's good practice for understanding data
print("--- 3. Exploratory Data Analysis ---")
print("Statistical summary of numerical features:")
print(df.describe())

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.tight_layout()
plt.savefig('correlation_matrix.png')
print("\nCorrelation matrix saved as 'correlation_matrix.png'.")
print("-" * 40 + "\n")

# --- 4. Model Training ---
print("--- 4. Model Training ---")

# Define features (X) and target (y)
X = df.drop('Selling_Price', axis=1)
y = df['Selling_Price']

# Split the data into training and testing sets
# test_size=0.2 means 20% of the data will be used for testing, 80% for training
# random_state ensures reproducibility of the split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training features shape: {X_train.shape}")
print(f"Testing features shape: {X_test.shape}")
print(f"Training target shape: {y_train.shape}")
print(f"Testing target shape: {y_test.shape}")

# Initialize and train the Random Forest Regressor model
# n_estimators: The number of trees in the forest.
# random_state: Controls the randomness of the estimator.
model = RandomForestRegressor(n_estimators=100, random_state=42)
print("\nTraining the RandomForestRegressor model...")
model.fit(X_train, y_train)
print("Model training complete.")
print("-" * 40 + "\n")

# --- 5. Model Evaluation ---
print("--- 5. Model Evaluation ---")

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")
print("-" * 40 + "\n")

# --- 6. Visualize Results ---
print("--- 6. Visualizing Results ---")

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
# Plotting the ideal line where actual equals predicted
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel("Actual Selling Price")
plt.ylabel("Predicted Selling Price")
plt.title("Actual vs. Predicted Selling Price")
plt.grid(True)
plt.tight_layout()
plt.savefig('actual_vs_predicted_selling_price.png')
plt.show() # Display the plot

print("Actual vs. Predicted Selling Price plot saved as 'actual_vs_predicted_selling_price.png'.")
print("-" * 40 + "\n")

# --- 7. Feature Importance (Optional) ---
# This helps to understand which features contributed most to the predictions
print("--- 7. Feature Importance ---")
feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
print("\nFeature Importances:")
print(feature_importances)

plt.figure(figsize=(12, 7))
sns.barplot(x=feature_importances.values, y=feature_importances.index, palette='viridis')
plt.title('Feature Importances for Car Price Prediction')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.savefig('feature_importances.png')
plt.show()
print("Feature importances plot saved as 'feature_importances.png'.")
print("-" * 40 + "\n")

print("Project execution complete!")